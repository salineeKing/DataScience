{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 10 - Sentiment Analysis\n",
    "## Author - Salinee Kingbaisomboon\n",
    "### UW NetID: 1950831\n",
    "\n",
    "This assignment requires that you build a sentiment analysis classifier for a series of tweets.\n",
    "The data consists of a file \"twitter_data.csv\". The file contains 16,000 tweets with their respective score. The attributes are the sentences, and the score is either 4 (for positive) or 0 (for negative).\n",
    "\n",
    "Assignment Instructions\n",
    "1. Complete all questions below.\n",
    "2. Comment on the applicability of the model on future tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import string\n",
    "import re\n",
    "# import nltk # Get No module named '_sqlite3' error on the Virtual Lab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from nltk.corpus import stopwords # Get No module named '_sqlite3' error on the Virtual Lab\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer # Get No module named '_sqlite3' error on the Virtual Lab\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # To suppress warning\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare Functions used in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean the text\n",
    "def preprocess(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        if step == 'remove_non_ascii':\n",
    "            text = ''.join([x for x in text if ord(x) < 128])\n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'remove_punctuation':\n",
    "            punct_exclude = set(string.punctuation)\n",
    "            text = ''.join(char for char in text if char not in punct_exclude)\n",
    "        elif step == 'remove_numbers':\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "# Get No module named '_sqlite3' error on the Virtual Lab\n",
    "#         elif step == 'remove_stopwords':\n",
    "#             stops = stopwords.words('english')\n",
    "#             word_list = text.split(' ')\n",
    "#             text_words = [word for word in word_list if word not in stops]\n",
    "#             text = ' '.join(text_words)\n",
    "#         elif step == 'stem_words':\n",
    "#             lmtzr = WordNetLemmatizer()\n",
    "#             word_list = text.split(' ')\n",
    "#             stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "#             text = ' '.join(stemmed_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate the word cloud data frame\n",
    "def generate_word_cloud(data, cutoff, sentiment):\n",
    "    # Filter the data sorce based on the sentiment (0 = positive, 1 = negative)\n",
    "    tweet_data = data[data.sentiment_label == sentiment].values.tolist()\n",
    "    clean_texts = data[data.sentiment_label == sentiment].clean_tweet\n",
    "    \n",
    "    # Create a document storage matrix\n",
    "    docs = {}\n",
    "    for ix, row in enumerate(clean_texts):\n",
    "        docs[ix] = row.split(' ')\n",
    "        \n",
    "    # We want to keep track of how many unique words there are:\n",
    "    num_nonzero = 0\n",
    "    vocab = set()\n",
    "    for word_list in docs.values():\n",
    "        unique_terms = set(word_list)    # all unique terms of this tweet\n",
    "        vocab.update(unique_terms)       # set union: add unique terms of this tweet\n",
    "        num_nonzero += len(unique_terms) # add count of unique terms in this tweet\n",
    "\n",
    "    doc_key_list = list(docs.keys())\n",
    "    \n",
    "    # Need to convert everything to a numpy array: (for quicker calculation)\n",
    "    doc_key_list = np.array(doc_key_list)\n",
    "    vocab = np.array(list(vocab))\n",
    "    \n",
    "    # We should keep track of how the vocab/term indices map to the matrix so that we can look them up later.\n",
    "    vocab_sorter = np.argsort(vocab)\n",
    "    \n",
    "    # Initialize our sparse matrix:\n",
    "    num_docs = len(doc_key_list)\n",
    "    vocab_size = len(vocab)\n",
    "    # A COO matrix is just a tuple of data, row indices, and column indices. Everything else is assumed to be zero.\n",
    "    data = np.empty(num_nonzero, dtype=np.intc)     # all non-zero\n",
    "    rows = np.empty(num_nonzero, dtype=np.intc)     # row index\n",
    "    cols = np.empty(num_nonzero, dtype=np.intc)     # column index\n",
    "    \n",
    "    ix = 0\n",
    "    print('Computing full term-document matrix (sparse), please wait!')\n",
    "    for doc_key, terms in docs.items():\n",
    "        # Find indices where elements should be inserted to maintain order\n",
    "        term_indices = vocab_sorter[np.searchsorted(vocab, terms, sorter=vocab_sorter)]\n",
    "\n",
    "        # count the unique terms of the document and get their vocabulary indices\n",
    "        uniq_indices, counts = np.unique(term_indices, return_counts=True)\n",
    "        n_vals = len(uniq_indices)  # = number of unique terms\n",
    "        ix_end = ix + n_vals # Add count to index\n",
    "\n",
    "        data[ix:ix_end] = counts                  # save the counts (term frequencies)\n",
    "        cols[ix:ix_end] = uniq_indices            # save the column index: index in \n",
    "        doc_ix = np.where(doc_key_list == doc_key)   # get the document index for the document name\n",
    "        rows[ix:ix_end] = np.repeat(doc_ix, n_vals)  # save it as repeated value\n",
    "\n",
    "        ix = ix_end  # resume with next document -> will add future data on the end.\n",
    "    \n",
    "    # Create the sparse matrix!\n",
    "    doc_term_mat = coo_matrix((data, (rows, cols)), shape=(num_docs, vocab_size), dtype=np.intc)\n",
    "    \n",
    "    # Trimming the Doc-term matrix\n",
    "    word_counts = doc_term_mat.sum(axis=0)\n",
    "    \n",
    "    # Look at how many words are above a specific cutoff\n",
    "    word_count_list = word_counts.tolist()[0]\n",
    "    \n",
    "    # Find which column indices are above cutoff\n",
    "    col_cutoff_ix = [ix for ix, count in enumerate(word_count_list) if count > cutoff]\n",
    "    \n",
    "    # Get the trimmed vocabulary\n",
    "    vocab_trimmed = np.array([vocab[x] for x in col_cutoff_ix])\n",
    "\n",
    "    # Re-do the vocab-sorter\n",
    "    vocab_sorter_trimmed = np.argsort(vocab_trimmed)\n",
    "    \n",
    "    # Trim the document-term matrix\n",
    "    doc_term_mat_trimmed = doc_term_mat.tocsc()[:,col_cutoff_ix]\n",
    "        \n",
    "    # Plot trimmed vocabulary frequency\n",
    "    trimmed_word_counts = doc_term_mat_trimmed.sum(axis=0)\n",
    "    trimmed_word_list = trimmed_word_counts.tolist()[0]\n",
    "    hist_breaks = np.arange(0, 500, 5)\n",
    "    plt.hist(trimmed_word_list, bins = hist_breaks)\n",
    "    plt.title('Histogram of trimmed word counts < 500')\n",
    "    plt.show()\n",
    "    \n",
    "    # Placeholder for return data frame\n",
    "    data_df = pd.DataFrame()\n",
    "  \n",
    "    # Find which tweets contain word:\n",
    "    appended_word = []\n",
    "    appended_word_count = []\n",
    "\n",
    "    # Look for how may indident this trimmed vocabulary appear on the tweet\n",
    "    for x in  vocab_sorter_trimmed:\n",
    "        doc_ix_with_word = []\n",
    "        word_of_interest = vocab_trimmed[vocab_sorter_trimmed[x]] # get the word\n",
    "        appended_word.append(word_of_interest)\n",
    "        vocab_interesting_ix = list(vocab).index(word_of_interest) # get index the word from vocabulary list\n",
    "        for ix, row in enumerate(tweet_data): # look into the tweet data\n",
    "            if word_of_interest in row[1]: # if found match\n",
    "                doc_ix_with_word.append(ix) # add indident index\n",
    "        appended_word_count.append(len(doc_ix_with_word)) # count the total index to get the number of occurance\n",
    "    \n",
    "    data_df['word'] = appended_word\n",
    "    data_df['word_count'] = appended_word_count \n",
    "    \n",
    "    print('Done!')\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment_label                                         tweet_text  \\\n",
      "0                1  @elephantbird Hey dear, Happy Friday to You  A...   \n",
      "1                1  Ughhh layin downnnn    Waiting for zeina to co...   \n",
      "2                0  @greeniebach I reckon he'll play, even if he's...   \n",
      "3                0              @vaLewee I know!  Saw it on the news!   \n",
      "4                0  very sad that http://www.fabchannel.com/ has c...   \n",
      "\n",
      "                                         clean_tweet  \n",
      "0  elephantbird hey dear happy friday to you alre...  \n",
      "1  ughhh layin downnnn waiting for zeina to cook ...  \n",
      "2  greeniebach i reckon hell play even if hes not...  \n",
      "3                  valewee i know saw it on the news  \n",
      "4  very sad that httpwwwfabchannelcom has closed ...  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>160000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment_label\n",
       "count    160000.000000\n",
       "mean          0.500000\n",
       "std           0.500002\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.500000\n",
       "75%           1.000000\n",
       "max           1.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read files\n",
    "url = \"https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/twitter_data.csv\"\n",
    "df = pd.read_csv(url, sep=\",\")\n",
    "df.columns = [\"sentiment_label\",\"tweet_text\"]\n",
    "\n",
    "# But sentiment is either '4' or '0'. We'll change that to '1' or '0' to indicate positive or negative sentiment.\n",
    "df.sentiment_label=df.sentiment_label.replace(4,1)\n",
    "    \n",
    "# Clean tweets\n",
    "steps = ['lowercase', 'remove_punctuation', 'remove_numbers', 'strip_whitespace']\n",
    "\n",
    "df['clean_tweet'] = df['tweet_text'].map(lambda s: preprocess(s, steps))\n",
    "\n",
    "# Check the Data Frame for first five rows\n",
    "print(df.head())\n",
    "\n",
    "# Check the Data Frame's statistics summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Generate word cloud for positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full term-document matrix (sparse), please wait!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fast</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dont</td>\n",
       "      <td>1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jonasbrothers</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>think</td>\n",
       "      <td>2555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shower</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  word_count\n",
       "0           fast         416\n",
       "1           dont        1123\n",
       "2  jonasbrothers          32\n",
       "3          think        2555\n",
       "4         shower         205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set cutoff = 50 since we'r targeting the popular word\n",
    "positive_word_cloud = generate_word_cloud(data=df, cutoff=50, sentiment=0) # sentiment = 0 for positive\n",
    "# Look for first 5 rows of the dataframe\n",
    "positive_word_cloud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Generate word cloud for negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing full term-document matrix (sparse), please wait!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first</td>\n",
       "      <td>803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>easier</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l</td>\n",
       "      <td>64622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thursday</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cheers</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  word_count\n",
       "0     first         803\n",
       "1    easier          57\n",
       "2         l       64622\n",
       "3  thursday          49\n",
       "4    cheers          56"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set cutoff = 50 since we'r targeting the popular word\n",
    "negative_word_cloud = generate_word_cloud(data=df, cutoff=50, sentiment=1) # sentiment = 1 for negative\n",
    "# Look for first 5 rows of the dataframe\n",
    "negative_word_cloud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Split data into 70% for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the TFIDF vectorizer.\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, max_features=6228, stop_words='english')\n",
    "\n",
    "# Fit the vectorizer over the dataset\n",
    "clean_texts = df['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into train-test. Please wait!\n",
      "\u001b[1m\u001b[4m30% Testing Data Set: \u001b[0m\n",
      "48000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "tweet_data = df.values.tolist()\n",
    "\n",
    "print('Splitting into train-test. Please wait!')\n",
    "\n",
    "# Get number of 30% training from the total tweet\n",
    "n_30_percent_testing = int(len(tweet_data)*0.3)\n",
    "\n",
    "print(f'\\033[1m\\033[4m30% Testing Data Set: \\033[0m')\n",
    "print(n_30_percent_testing)\n",
    "\n",
    "y_targets = np.array([y[0] for y in tweet_data])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets,\n",
    "                                                    y_targets,\n",
    "                                                    test_size=n_30_percent_testing,\n",
    "                                                    random_state=42)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Build a classifier that classifies the sentiment of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting a standard Logistic Model training!\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Starting a standard Logistic Model training!')\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: What is the accuracy of your model when applied to testing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7812767857142857\n",
      "Test accuracy: 0.7554375\n"
     ]
    }
   ],
   "source": [
    "## Compute results on the train and test set\n",
    "train_probs = lr.predict_proba(X_train)\n",
    "train_results = np.argmax(train_probs, axis=1)\n",
    "\n",
    "test_probs = lr.predict_proba(X_test)\n",
    "test_results = np.argmax(test_probs, axis=1)\n",
    "\n",
    "# Compute accuracies\n",
    "train_logical_correct = [pred == actual for pred, actual in zip(train_results, y_train)]\n",
    "train_acc = np.mean(train_logical_correct)\n",
    "\n",
    "test_logical_correct = [pred == actual for pred, actual in zip(test_results, y_test)]\n",
    "test_acc = np.mean(test_logical_correct)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[17488  6556]\n",
      " [ 5183 18773]]\n",
      "===================================\n",
      "             Class 1   -   Class 0\n",
      "Precision: [0.77138194 0.74116625]\n",
      "Recall   : [0.72733322 0.78364502]\n",
      "F1       : [0.74871026 0.76181394]\n",
      "Support  : [24044 23956]\n"
     ]
    }
   ],
   "source": [
    "# Computes the precision, recall and Fscore of the model for positive and negative tweets\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, test_results)\n",
    "\n",
    "# Get the parts of the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_results).ravel()\n",
    "\n",
    "# Print results\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, test_results))\n",
    "print('='*35)\n",
    "print('             Class 1   -   Class 0')\n",
    "print('Precision: {}'.format(precision))\n",
    "print('Recall   : {}'.format(recall))\n",
    "print('F1       : {}'.format(f1))\n",
    "print('Support  : {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: What conclusions can you draw from the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the results:\n",
    "1. From **Confusion Matrix**: \n",
    "    - **True Positive = 18,773** and **True Negative = 17,488**\n",
    "    - **False Negative (Type II error) = 5,183** and **False Positive (Type I error) = 6,556** \n",
    "2. **Precision** when we predict if this word has either positive or negative sentiment are almost the same (+70%) which mean when we predict 100 times, we will get it correct 70 times.\n",
    "3. **Recall** for both class 1 (negative) and class 0 (positive) are pretty good which mean we have the useful proportion of both cases about +70% amoung the overall data.\n",
    "4. **F1** for both class 1 (negative) and class 0 (positive) are pretty good and our data sorce has pretty ok even class distribution for both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Is it better to have a model per source?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different data sources have different characteristic and bias. For example, if this model is for **Classification**, different data source might not yield the same accuracy result. Let say, in our assignment, our data source is from **tweet** which can have a lot of **typos** and special meaning text such as **emoji**. The model to train this will be different if the data source came from a **textbook** which has less typos and prone to use more **latin words**. The model which is work very well on the tweet might not work well on the data source from the textbook since the nature of the language are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
